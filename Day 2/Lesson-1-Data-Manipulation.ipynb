{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "higher-beaver",
   "metadata": {},
   "source": [
    "# Lesson 1: Data Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-beatles",
   "metadata": {},
   "source": [
    "## 0.1 Python Modules & Packages\n",
    "\n",
    "**Modules** in Python are simply python files that contain functions, objects, etc. designed to perform a specifc function. \n",
    "\n",
    "**Packages** in Python refer to a collection of multiple modules that are grouped together. \n",
    "\n",
    "There are currently > 200,000 publicly available Python packages on the official python package list and utilizing these packages in your Python code will allow you to easily perform a variety tasks. \n",
    "\n",
    "Some common packages are:\n",
    "- `os` -> Operating system related tasks\n",
    "- `numpy` -> Fundamental package for scientific computing in Python\n",
    "- `matplotlib.pyplot` -> Basic Plotting Module of the matplotlib package\n",
    "- `seaborn` -> Fancy plotting\n",
    "- `pandas` -> Data manipulation and analysis\n",
    "- `scikit-learn` -> Advanced data analysis\n",
    "\n",
    "Before you can use a package, you must install the package on your computer. However, Google Colab has all packages already installed availalbe for use.\n",
    "To use a package in your code, you must first import the package which can be done a couple different ways using the `import` command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-egypt",
   "metadata": {},
   "source": [
    "You can import the entire package using the following syntax \n",
    "   \n",
    "    import <package_name>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the entire os package\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-shield",
   "metadata": {},
   "source": [
    "Packages with names more than a few letters are typically given an **alias**\n",
    "\n",
    "    import <package_name> as <alias>\n",
    "    \n",
    "Many common packages have conventional aliases.\n",
    "- `numpy` -> `np`\n",
    "- `matplotlib.pyplot` -> `plt`\n",
    "- `seaborn` -> `sns`\n",
    "- `pandas` -> `pd`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages using an alias \n",
    "import numpy as np # Imports NumPy and gives it the name \"np\"\n",
    "import pandas as pd # Imports Pandas and gives it the name \"pd\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-blues",
   "metadata": {},
   "source": [
    "You can also import a specific module of a package. This avoids importing the entire package and allows you to alias just that module.\n",
    "\n",
    "    import <package_name>.<module_name> as <alias>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plotting module of the Matplotlib package and give it a short alias\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-board",
   "metadata": {},
   "source": [
    "Lastly, you can import only specfic functions from a package or module to avoid importing and entire package\n",
    "\n",
    "    from <package_name>.<module_name> import <function1>, <function2>, ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the isdir() and isfile() functions from the os.path module\n",
    "from os.path import isdir, isfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-lighting",
   "metadata": {},
   "source": [
    "For best practices: \n",
    "- All import statements should be place together at the top of your file\n",
    "- Each package should be on a different line \n",
    "- Import statements are organized from most broad to most specfic \n",
    "    - Package imports \n",
    "    - Module imports \n",
    "    - Function imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages with proper formatting\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isdir, isfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-vintage",
   "metadata": {},
   "source": [
    "## 0.2 Review of basic `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-jimmy",
   "metadata": {},
   "source": [
    "NumPy arrays are efficent ways to store data that have many useful properties. \n",
    "Values inside of `np.array` objects can be selected the same as python lists and NumPy arrays have built in functions like `.mean()` and `.max()` that easily perform array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,5,6,6])\n",
    "\n",
    "print(a[0])\n",
    "print(a[1:3])\n",
    "print(a.mean())\n",
    "print(a.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-liberal",
   "metadata": {},
   "source": [
    "Additonally, the `numpy` package has other functions that can be applied to arrays such as `np.unique()` which returns an array without any repeat values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-congo",
   "metadata": {},
   "source": [
    "You can perform numerical operations on arrays to generate new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new array by adding 3 to the previous array\n",
    "b = a + 3\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-pregnancy",
   "metadata": {},
   "source": [
    "In addition to numerical operations you can perform conditional operations on an array. \n",
    "\n",
    "Below we check to see which values are less than 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return whether each element is less than 4\n",
    "a < 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-classroom",
   "metadata": {},
   "source": [
    "By placing conditional statements inside of `[]` we can select the values of the array where the conditional statement is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return an array of the elements that are less than 4\n",
    "a[a < 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-condition",
   "metadata": {},
   "source": [
    "## Section 1: Data manipulation with `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-throat",
   "metadata": {},
   "source": [
    "### 1.1 The `DataFrame`\n",
    "\n",
    "Pandas stores data in tables called `DataFrames`. \n",
    "\n",
    "<img src=\"https://pandas.pydata.org/docs/_images/01_table_dataframe.svg\" height=300px width=auto>\n",
    "\n",
    "Each `DataFrame` is made of **rows** and **columns** of data. \n",
    "\n",
    "The header of the `DataFrame` contains the name of each column.\n",
    "\n",
    "The `Index` of the `DataFrame` contains a unique name for each row. The `Index` defaults to row numbers but can be anything (Dates, Strings,etc.) as long as it is unique for each row.\n",
    "\n",
    "Each column of a `DataFrame` is stored as a `numpy.array`. You can make a `DataFrame` manually by specifying the column names and the values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple DataFrame \n",
    "df = pd.DataFrame({\n",
    "    'A': np.array([1,2,3]),\n",
    "    'B': np.array([4,5,6]),\n",
    "    'C': np.array(['do','re','mi'])\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-demonstration",
   "metadata": {},
   "source": [
    "Data stored in a `pandas` object can be returned to a `numpy.array` using the `to_numpy()` function.\n",
    "\n",
    "To turn the entire `DataFrame` back into a `numpy.array` would use the following syntax:\n",
    "\n",
    "    df.to_numpy()\n",
    "    \n",
    "**Note:** This will not save any information in the `Index` or column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the DataFrame into a 2D np.array\n",
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-rocket",
   "metadata": {},
   "source": [
    "### 1.2 Loading data from a file\n",
    "\n",
    "You can read/write data to/from a `DataFrame` using a variety of file types\n",
    "\n",
    "<img src=\"https://pandas.pydata.org/docs/_images/02_io_readwrite.svg\" height=500px width=auto>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-terror",
   "metadata": {},
   "source": [
    "To create a data frame from a file you can use the following syntax\n",
    "\n",
    "    df = pd.read_<file-type>(file_path)\n",
    "    \n",
    "For example to read data from a .csv file you would use\n",
    "\n",
    "    df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from country_vaccinations.csv into a DataFrame\n",
    "df = pd.read_csv(\"data/country_vaccinations.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-cutting",
   "metadata": {},
   "source": [
    "### 1.3 Viewing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-chicken",
   "metadata": {},
   "source": [
    "You can get a summary of the column names and how many non-empty rows each column has using the `.info()` function\n",
    "\n",
    "    df.info()\n",
    "    \n",
    "This is a useful first step to become familiar with the column names and ensure your data loaded correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the number of rows and columns in the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-respect",
   "metadata": {},
   "source": [
    "The `DataFrame.head()` and `DataFrame.tail()` functions to view the top and bottoms rows of the `DataFrame`. \n",
    "\n",
    "The default is 5 rows, but you can optionally specify the number of rows you would like to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 5 rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the bottom 10 rows of the DataFrame\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-drinking",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The `DataFrame.describe()` method will report summary statistics for all numerical columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows summary statistics for all of the numerical columns in the DataFrame\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-acrylic",
   "metadata": {},
   "source": [
    "In the next lesson we will cover how to make more complex and better formatted plots. However, often times it can be helpful to make simple plots of your data. This can be done easily in using the `DataFrame.plot` method\n",
    "\n",
    "    df.plot(x=column_1, y=column_2, kind=<plot_type>)\n",
    "    \n",
    "The `x` and `y` arguments are the column names to place on the x-axis and y-axis. The `kind` argument specifies the type of plot you want to creat. \n",
    "\n",
    "You can check the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) for a complete list of plot types, but a couple common options for the `kind` argument are:\n",
    "\n",
    "- `line` -> line plot (default)\n",
    "- `bar` -> vertical bar plot\n",
    "- `hist` -> histogram\n",
    "- `scatter` -> scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y=\"daily_vaccinations_per_million\",kind=\"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-emission",
   "metadata": {},
   "source": [
    "### 1.4 Selecting Data by Position\n",
    "\n",
    "Similar to `numpy.array` you can easily select subsets of your `DataFrame` based on their location in the `DataFrame`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-optics",
   "metadata": {},
   "source": [
    "To select an entire column use `[]` with the column name as a string. \n",
    "\n",
    "    df[column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column\n",
    "df[\"country\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-session",
   "metadata": {},
   "source": [
    "To locate specific rows use the `DataFrame.loc[]` and `DataFrame.iloc[]` methods\n",
    "\n",
    "Using `.loc[]` uses the names of the rows as listed in the `Index`\n",
    "\n",
    "    df.loc[index_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the row at Index 20\n",
    "df.loc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows with Index names 20-25\n",
    "df.loc[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-blackjack",
   "metadata": {},
   "source": [
    "Using `.iloc[]` uses the row and column numbers instead of the name\n",
    "\n",
    "    df.iloc[index_number]\n",
    "\n",
    "**Note:** When using `.iloc[]` with slicing the last number is not included, but when using `.loc[]` the last number is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row numbers 20-25\n",
    "df.iloc[20:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-space",
   "metadata": {},
   "source": [
    "To locate specific rows and columns, tou can use the `DataFrame.loc[]` and `DataFrame.iloc[]` methods with both index names/numbers and column names/numbers\n",
    "\n",
    "Using `.loc[]` with the names of the columns and rows:\n",
    "\n",
    "    df.loc[index_name,column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show rows 1000-1010 and columns \"country\" and date\n",
    "df.loc[1000:1010,[\"country\",\"date\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-victory",
   "metadata": {},
   "source": [
    "Using `.iloc[]` with the row and column numbers instead of the name:\n",
    "\n",
    "    df.iloc[index_number,column_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows rows 1000-1010 and the columns at index 0 and 2. Should be the same as the previous example\n",
    "df.iloc[1000:1011,[0,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-artwork",
   "metadata": {},
   "source": [
    "### 1.5 Selecting Data by Condition\n",
    "\n",
    "Similar to conditional indexing in `numpy`, you can also select values from a `DataFrame` if they meet specific conditions.\n",
    "\n",
    "#### 1.5.1 Selecting with a Single Condition\n",
    "By placing a single conditional statement inside of `[]` you will select only **rows** where that condition is true.\n",
    "\n",
    "For example, the following syntax will select all the rows where a specific column is equal to a specfic value.\n",
    "\n",
    "    df[df[column_name] == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects every row where the date is June 25th, 2021\n",
    "df[df[\"date\"] == \"2021-06-25\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-attempt",
   "metadata": {},
   "source": [
    "#### Group Practice: USA Vaccination Data\n",
    "\n",
    "Fill in the code to create a new `DataFrame` with just data from the United States. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the missing code to select the rows with data from the United States\n",
    "# Selects all rows where the country name is United States and saves them as a separate database\n",
    "usa_df = df[df[\"country\"] == \"United States\"]\n",
    "\n",
    "usa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-utilization",
   "metadata": {},
   "source": [
    "Use `usa_df.loc` or `usa_df.iloc` to find out how many more people were fully vaccinated in the US on the 200th day than on the 100th day? Assume the rows are in order by date and each row is one day apart from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .iloc[]\n",
    "# Difference in fully vaccinated people on the 200th day vs the 100th day\n",
    "usa_df.iloc[199,5] - usa_df.iloc[99,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using .loc[]\n",
    "# Difference in fully vaccinated people on the 200th day vs the 100th day\n",
    "usa_df.loc[29225,\"people_fully_vaccinated\"] - usa_df.loc[29125,\"people_fully_vaccinated\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-guidance",
   "metadata": {},
   "source": [
    "Notice here when creating a new `DataFrame` from an existing `DataFrame` all of the information is retained, including the `Index`. \n",
    "\n",
    "This means that the following code would **error** if ran as there isn't a row with `Index` name of 199 in the `usa_df` `DataFrame`.\n",
    "\n",
    "    usa_df.loc[199]\n",
    "    \n",
    "However this code would run and return the 200th row\n",
    "    \n",
    "    usa_df.iloc[199]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-compromise",
   "metadata": {},
   "source": [
    "#### 1.5.2 Selecting with Multiple Condtions\n",
    "\n",
    "You can use more than one conditional statement to further refine your selection.\n",
    "\n",
    "To do this, place `()` around each conditional statement and use the following logical operators to separate each statement.\n",
    "- `&` -> `and` \n",
    "- `|` -> `or`\n",
    "- `~` -> `not`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows in the usa_df where the daily vaccinations were above 2,000,000 and the total vaccination were above 250,000,000\n",
    "# Separting each condition with a newline can help make the code more readable\n",
    "usa_df[(usa_df['daily_vaccinations'] > 2000000) & \n",
    "       (usa_df['total_vaccinations'] > 250000000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-contest",
   "metadata": {},
   "source": [
    "#### 1.5.3 Conditional Selection for Mulitple Possible Values\n",
    "\n",
    "Another useful ways to select rows are if the values are part of some set of values. \n",
    "\n",
    "The `.isin()` function selects values if they are contained within a list of possible values.\n",
    "\n",
    "    df[column_name].isin(value_list)\n",
    "    \n",
    "This allows us to select for multiplt potential values rather than having several `or` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of north american countries of interest\n",
    "north_amer_countries = [\"Canada\", \"United States\", \"Mexico\"]\n",
    "\n",
    "# select only the rows where the country column has a value in the list above\n",
    "df[(df[\"country\"].isin(north_amer_countries))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-walnut",
   "metadata": {},
   "source": [
    "### 1.6 Concat & Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-jewel",
   "metadata": {},
   "source": [
    "#### 1.6.1 Concat\n",
    "Concatenating `DataFrames` is the process of taking two or more `DataFrames` and stacking their rows together into one `DataFrame`. \n",
    "\n",
    "An Example is shown below: \n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/finalmerge1.jpg\" width=1000>\n",
    "\n",
    "The `pd.concat` function takes in a list of `DataFrames`, concatenates them by stacking the rows together and returns them as a single `DataFrame`.\n",
    "\n",
    "    new_df = pd.concat([df_1, df_2, df_3...])\n",
    "    \n",
    "This can only be done if the `DataFrames` have the same columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.iloc[900:903,:]\n",
    "df_2 = df.iloc[1900:1903,:]\n",
    "df_3 = df.iloc[30000:30003,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.concat([df_1,df_2,df_3])\n",
    "\n",
    "cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-stable",
   "metadata": {},
   "source": [
    "For `DataFrame` objects which don’t have a meaningful index, you may wish to append them and ignore the fact that they may have overlapping indexes or reset the indexes to the default numbering. \n",
    "\n",
    "To do this, use the `ignore_index` argument:\n",
    "\n",
    "    new_df = pd.concat([df_1, df_2, df_3...], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-branch",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df_no_ind = pd.concat([df_1,df_2,df_3],ignore_index=True)\n",
    "\n",
    "cat_df_no_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-cancer",
   "metadata": {},
   "source": [
    "#### 1.6.2 Merge\n",
    "\n",
    "Merging data involves taking two separate datasets and combining their columns. To do this, rows are matched together by matching the values in one column that is common to both datasets.\n",
    "\n",
    "And example is shown here, where the `item_id` column is used to match the data in both datasets\n",
    "<img src=\"https://absentdata.com/wp-content/uploads/2019/07/pd.merge-1.png\" height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-payday",
   "metadata": {},
   "source": [
    "The `pd.merge()` function takes care of this process and uses the following syntax:\n",
    "\n",
    "    merge_df = pd.merge(left,right,how=\"inner\",on=None)\n",
    "    \n",
    "Here the `left` and `right` arguments refer to the two `DataFrame` objects we want to merge. The `how` argument specifies the type of merge we want to do with the default being `inner`. The `on` argument refers to the column we want to use to match. If you don't specify a column name with `on` then by default the `Index` will be used to match values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-tackle",
   "metadata": {},
   "source": [
    "There are 4 different ways to specifiy `how` to merge data:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*rMW6TQfoBwfOZrFhTdxUVg.png\" width=500>\n",
    "\n",
    "\n",
    "- `inner` selects only the rows of data where there is the same value in both datasets.\n",
    "- `outter` selects all rows from both datasets, matching where possible and leaving missing values otherwise\n",
    "- `left` selects all the rows from the left dataset and only the rows from the right dataset where there are matches\n",
    "- `right` selects all the rows from the right dataset and only the rows from the left dataset where there are matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-improvement",
   "metadata": {},
   "source": [
    "#### Group Exercise: Merging `DataFrames`\n",
    "\n",
    "The goal of this exercise is to merge our existing vaccination dataset with a new dataset that contains information on the GDP of the varioius countires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-image",
   "metadata": {},
   "source": [
    "The first step is to load in the dataset which is located at `\"data/country_gdp.csv\"`. However, if you attempt to run `pd.read_csv(\"data/country_gdp.csv\")` you will get an error.\n",
    "\n",
    "If we look at our .csv file we can see that the file starts with the following lines\n",
    "\n",
    "    0\t﻿\"Data Source\"\tWorld Development Indicators\t\n",
    "    1\t\t\t\n",
    "    2\tLast Updated Date\t2023-07-25\t\n",
    "    3\t\t\t\n",
    "    4\tCountry Name\tCountry Code\tIndicator Name\",\"Indicator Code\",\"1960\",\"1961\",\"1962\",\"1963\",\"1964\",\"1965\",\"1966\",\"1967\",\"1968\",\"1969\",\"1970\",\"1971\",\"1972\",\"1973\",\"1974\",\"1975\",\"1976\",\"1977\",\"1978\",\"1979\",\"1980\",\"1981\",\"1982\",\"1983\",\"1984\",\"1985\",\"1986\",\"1987\",\"1988\",\"1989\",\"1990\",\"1991\",\"1992\",\"1993\",\"1994\",\"1995\",\"1996\",\"1997\",\"1998\",\"1999\",\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\",\"2022\"\n",
    "    5\tAruba\tABW\tGDP (current US$)\",\"NY.GDP.MKTP.CD\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\",\"405586592.178771\",\"487709497.206704\",\"596648044.692737\",\"695530726.256983\",\"764804469.273743\",\"872067039.106145\",\"958659217.877095\",\"1083240223.46369\",\"1245810055.86592\",\"1320670391.06145\",\"1379888268.15642\",\"1531843575.41899\",\"1665363128.49162\",\"1722905027.93296\",\"1873184357.5419\",\"1896648044.69274\",\"1962011173.18436\",\"2044134078.21229\",\"2254748603.35196\",\"2359776536.31285\",\"2469832402.23464\",\"2677653631.28492\",\"2843016759.77654\",\"2553631284.9162\",\"2453631284.9162\",\"2637988826.81564\",\"2615083798.88268\",\"2727932960.89385\",\"2791061452.51397\",\"2963128491.62011\",\"2983798882.68156\",\"3092178770.94972\",\"3276187709.49721\",\"3395793854.7486\",\"2610038938.54749\",\"3126019385.47486\",\"\n",
    "    \n",
    "Here the header row is actually on line 4 and the first couple of rows contain metadata about the dataset.\n",
    "\n",
    "Go to the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) for `pd.read_csv` and figure out how to load the `.csv` file such that the header row is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in missing code to load the data in country_gdp.csv\n",
    "gdp_df = # Fill in this line\n",
    "\n",
    "gdp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-bahamas",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "To skip the extra rows and fix your error you can use the syntax\n",
    "\n",
    "    df = pd.read_csv(file_path,skiprows=num_rows)\n",
    "    \n",
    "Or you can specify the row number that contains the header\n",
    "\n",
    "    df = pd.read_csv(file_path,header=header_number)\n",
    "    \n",
    "**Note**: Empty rows in your data **don't count** towards the row number for the `header` argument, but they **do count** towards rows to skip for the `skiprows` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are equivalent ways to load the csv file with gdp data\n",
    "# gdp_df = pd.read_csv(\"data/country_gdp.csv\",skiprows=4) # Load gdp data skipping the first 4 rows in csv file\n",
    "\n",
    "gdp_df = pd.read_csv(\"data/country_gdp.csv\",header=2) # Load gdp data using the row at index 2 as the header \n",
    "\n",
    "gdp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-metallic",
   "metadata": {},
   "source": [
    "Next, create a new `DataFrame` that contains only the columns `Country Code` and `2020`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the code to select only the country code and the data from 2020 \n",
    "gdp_2020 = \n",
    "\n",
    "gdp_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-settle",
   "metadata": {},
   "source": [
    "To make merging the data easier we are going to rename the Country Code column so that it matches the name of the country code column in our other `DataFrame`.\n",
    "\n",
    "Additionally, we can rename the column with the gdp values to be \"gdp\" so that it will be clear what the data in the column means after we merge. \n",
    "\n",
    "Run the code block below to rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns to match vaccination dataset and provide clarity\n",
    "gdp_2020.columns = [\"iso_code\",\"gdp\"]\n",
    "\n",
    "gdp_2020.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-print",
   "metadata": {},
   "source": [
    "Finally, create a new `DataFrame` by merging the vaccination data in `df` with the gdp data in `gdp_2020`. Use the `iso_code` column to match the datasets and an `inner` join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the code to merge the gdp data with our vaccination data by matching country codes \n",
    "merged_df = \n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-custom",
   "metadata": {},
   "source": [
    "### 1.7 Group by\n",
    "\n",
    "**Group by** refers to a process involving one or more of the following steps:\n",
    "\n",
    "- Splitting the data into groups based on some criteria\n",
    "- Applying a function to each group independently\n",
    "- Combining the results into a data structure\n",
    "\n",
    "We will show a very basic example of how to do these steps below, but for more complex grouping examples check out the `pandas` [grouping documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#groupby)\n",
    "\n",
    "The `.groupby` function will perform the grouping and is typically combined with the function you wish to apply to the groups.\n",
    "\n",
    "    df.groupby(column_name).<function_to_apply>\n",
    "    \n",
    "In the example below, we will group the `DataFrame` by each country, and then apply the `max()` function to get the maximum value of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the country name and apply the max() function to each column\n",
    "df.groupby('country').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-buffalo",
   "metadata": {},
   "source": [
    "The output that results contains the same columns as our original `DataFrame`, but the `Index` is now the country name and each column contains only the maximum value from the previous `DataFrame`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-arrangement",
   "metadata": {},
   "source": [
    "You can also use `groupby` and apply one or more functions to only specific columns\n",
    "\n",
    "    df.groupby(column_to_group)[column_to_apply].<function_to_apply>\n",
    "    \n",
    "Below we will do the same grouping, but only find the maximum value of one column as well as sort the results from highest to lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the country name, select only one column, apply the max() function, and then apply the sort_values function\n",
    "clean_df.groupby('country')[\"people_fully_vaccinated_per_hundred\"].max().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-notification",
   "metadata": {},
   "source": [
    "Similar to a `DataFrame`, you can use the `.plot` function to make quick plots of your `groupby` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by the country name, select only one column, apply the max() function, and then apply the sort_values function\n",
    "vax_per_ctry = clean_df.groupby('country')[\"people_fully_vaccinated_per_hundred\"].max().sort_values(ascending=False)\n",
    "\n",
    "vax_per_ctry.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-terrorism",
   "metadata": {},
   "source": [
    "### 1.8 Missing Data\n",
    "\n",
    "Often times when working with datasets you will encounter rows that are missing data for some or all of the columns. \n",
    "\n",
    "Pandas will fill in those values with `NaN` which is the `np.nan` object. `NaN` stands for \"Not a Number\".\n",
    "\n",
    "Helpful `pandas` methods to deal with missing data are:\n",
    "\n",
    "- `df.dropna()`      -> Removes rows with missing/NaN values\n",
    "- `df.fillna(value)` -> Replaces NaN with new values \n",
    "- `isna()`          -> Returns true for the values in the DataFrame that are NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-gallery",
   "metadata": {},
   "source": [
    "The vaccination data we have been working has no missing data. However, the originial dataset found in `country_vaccinations_raw.csv` is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"data/country_vaccinations_raw.csv\")\n",
    "\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-technician",
   "metadata": {},
   "source": [
    "You can drop all rows that are missing values using\n",
    "   \n",
    "    df_raw.dropna()\n",
    "\n",
    "Alternatively, you drop rows that are missing values in a certain column or columns using the `subset` argument\n",
    "\n",
    "    df_raw.dropna(subset=columns_to_drop)\n",
    "    \n",
    "    \n",
    "**Note:** `.dropna()` returns a new `DataFrame` with the missing values removed and does not alter the existing `DataFrame`. You can optionally specify the argument `inplace=True` in order to have it remove the values and alter the existing `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only rows that are missing values in the people_vaccinated OR \"people_fully_vaccinated\" OR daily_vaccinations columns\n",
    "# And save the output as a new DataFrame\n",
    "clean_df = df.dropna(subset=[\"people_vaccinated\",\"people_fully_vaccinated\",\"daily_vaccinations\"])\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-terrain",
   "metadata": {},
   "source": [
    "### 1.9 String methods\n",
    "\n",
    "If your column contains a string there are a variety of additional methods you can use to generate conditional statments or manipulate the strings.\n",
    "\n",
    "These methods are all available through the `.str` attribute used on a column that contains strings as shown below.\n",
    "\n",
    "    df[column_name].str.<str_method>\n",
    "    \n",
    "An example of using a `str` method for a conditions statement is using `str.contains()` to see if a string contains a particular substring\n",
    "\n",
    "    df[column_name].str.contains(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays rows where the country includes \"stan\"\n",
    "stan_df = df[df[\"country\"].str.contains(\"stan\")]\n",
    "\n",
    "# Display the name of each country in the DataFrame\n",
    "print(stan_df['country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-exploration",
   "metadata": {},
   "source": [
    "Examples of other common string methods that you can use with `.str` are:\n",
    "\n",
    "- `str.lower()`            -> Makes all letters lowercase\n",
    "- `str.upper()`            -> Makes all letters uppercase\n",
    "- `str.capitalize()`       -> Makes all words follow proper capitalization\n",
    "- `str.strip()`            -> Removes any leading or trailing whitespaces\n",
    "- `str.replace(str1,str2)` -> Replaces substring that match str1 with str2\n",
    "    \n",
    "You can chain together multiple `.str` methods to use them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of our DataFrame to change without changing original DataFrame\n",
    "# Note: Just using str_df = df will not make a copy and will still point both names to the dame DataFrame\n",
    "str_df = df.copy()\n",
    "\n",
    "# Takes the \"country\" column and; Removes leading/trailing white space, makes it lowercase, replaces all spaces with underscores \n",
    "str_df[\"country\"] = str_df[\"country\"].str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Show rows where the country contains an underscore \n",
    "str_df[str_df[\"country\"].str.contains(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-death",
   "metadata": {},
   "source": [
    "These methods can also be applied on the column names as well which can be useful to create consistant formatting amongst columns\n",
    "\n",
    "You can access the column names using \n",
    "\n",
    "    df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces the column names with properly captialized names separated by spaces\n",
    "# Note: in general it is bettet to have lower case column names with no spaces as it makes them easier to type without ambiguity\n",
    "str_df.columns = str_df.columns.str.replace(\"_\", \" \").str.capitalize()\n",
    "\n",
    "str_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
